import requests
import pandas as pd
import schedule
import time
from datetime import datetime, timedelta
import os
import json
import re
import numpy as np
from collections import defaultdict
from openai import OpenAI

# ================= âš™ï¸ é…ç½®åŒº =================
DATA_FILE_PATH = r"C:\Users\12398\Desktop\QAQ\8690project\trade_system_test1\rag_engine\news_data.csv"
DEEPSEEK_API_KEY = ""  # ğŸ”´ å¿…å¡«
BASE_URL = "https://api.deepseek.com"
POLLING_INTERVAL = 2
BACKFILL_COUNT = 60
# ================= ğŸ§  å…¨å±€çŠ¶æ€ =================
SEEN_NEWS_BUFFER = set()
MARKET_CONTEXT_BUFFER = []
MARKET_CONTEXT_MANUAL = []
SECTOR_HISTORY_BUFFER = []

# ================= ğŸ—ºï¸ äº§ä¸šé“¾åˆ†çº§å›¾è°± (Knowledge Graph) =================
# è¿™æ˜¯ç»™ AI çœ‹çš„â€œä½œæˆ˜åœ°å›¾â€ï¼ŒæŒ‡å¯¼å®ƒå¦‚ä½•ç²¾å‡†æ‰“æ ‡
SECTOR_KNOWLEDGE = """
ã€ä¸€çº§å¤§ç±»ã€‘ -> ã€äºŒçº§ç»†åˆ† (Sub-Sector)ã€‘
1. äººå·¥æ™ºèƒ½(AI) -> [AIç¡¬ä»¶(CPO/ç®—åŠ›/æœåŠ¡å™¨), AIåº”ç”¨(æ¸¸æˆ/ä¼ åª’/æ•™è‚²/Sora), AIæ¨¡å‹/æ•°æ®]
2. åŠå¯¼ä½“ -> [åŠå¯¼ä½“è®¾å¤‡(å…‰åˆ»æœº), åŠå¯¼ä½“ææ–™, èŠ¯ç‰‡è®¾è®¡, å°æµ‹/åˆ¶é€ ]
3. æ–°èƒ½æº -> [é”‚ç”µ/å›ºæ€ç”µæ± , å…‰ä¼, é£ç”µ, å‚¨èƒ½]
4. æ±½è½¦äº§ä¸šé“¾ -> [æ•´è½¦, æ±½é…/è‡ªåŠ¨é©¾é©¶, é£è¡Œæ±½è½¦(ä½ç©º)]
5. åŒ»è¯åŒ»ç–— -> [åˆ›æ–°è¯/CXO, ä¸­è¯, åŒ»ç–—å™¨æ¢°]
6. æ•°å­—ç»æµ -> [æ•°æ®è¦ç´ , ä¿¡åˆ›/å›½äº§è½¯ä»¶, ç®—åŠ›ç§Ÿèµ]
7. é‡‘è/åœ°äº§ -> [åˆ¸å•†, é“¶è¡Œ, æˆ¿åœ°äº§, ä¿é™©]
"""

# ================= ğŸ—‘ï¸ å™ªéŸ³é»‘åå• =================
NOISE_KEYWORDS = [
    "ç‰¹çº¦", "å¹¿å‘Š", "æŠ¥å", "å³°ä¼š", "è®ºå›", "å…è´£å£°æ˜",
    "ç‚¹å‡»æŸ¥çœ‹", "é£é™©æç¤º", "åŠ å…¥åœˆå­", "å¼€æˆ·", "ä¸Šä¿®",
    "å¤§å®—äº¤æ˜“", "èèµ„èåˆ¸", "é¾™è™æ¦œ", "æ±‡ç‡", "å€ºå¸‚"
]


# ================= ğŸ› ï¸ å·¥å…·å‡½æ•° =================
def clean_json_string(text):
    # å°è¯•1: åŒ¹é… Markdown ä»£ç å—
    match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
    if match: return match.group(1)

    # å°è¯•2: æš´åŠ›æŸ¥æ‰¾ç¬¬ä¸€ä¸ª '[' å’Œæœ€åä¸€ä¸ª ']'
    # è¿™èƒ½è§£å†³ AI åºŸè¯å¤šä½†åŒ…å« JSON çš„æƒ…å†µ
    start = text.find('[')
    end = text.rfind(']')
    if start != -1 and end != -1:
        return text[start:end + 1]

    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒåŸæ ·è¿”å›ï¼Œè®© json.loads å»æŠ¥é”™å¹¶æ‰“å°åŸå§‹å†…å®¹
    return text


def get_dynamic_half_life(dt):
    """åŠ¨æ€åŠè¡°æœŸï¼šäº¤æ˜“æ—¶æ®µåŠ é€Ÿè¡°å‡(4h)ï¼Œä¼‘å¸‚æ—¶æ®µå‘é…µ(24h)"""
    is_workday = dt.weekday() < 5
    hour_float = dt.hour + dt.minute / 60.0
    is_trading_time = is_workday and ((9.5 <= hour_float <= 11.5) or (13.0 <= hour_float <= 15.0))
    return 4.0 if is_trading_time else 24.0


def init_memory():
    global SEEN_NEWS_BUFFER, MARKET_CONTEXT_BUFFER
    if os.path.exists(DATA_FILE_PATH):
        try:
            df = pd.read_csv(DATA_FILE_PATH, encoding='utf-8-sig')
            SEEN_NEWS_BUFFER = set(df['content'].tolist())
            print(f"ğŸ“š è®°å¿†æ¢å¤: {len(SEEN_NEWS_BUFFER)} æ¡")
        except:
            print("âš ï¸ å†å²æ–‡ä»¶ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")


# ================= ğŸ“ æˆ˜ç•¥å†…å‚ç”Ÿæˆå™¨ (V14.0 ç»“æ„åŒ–ç‰ˆ) =================
def generate_daily_brief():
    print("\nâ˜€ï¸ æ­£åœ¨ç”Ÿæˆã€DeepQuant ç»“æ„åŒ–å†…å‚ (V14.0)ã€‘...")

    if not os.path.exists(DATA_FILE_PATH):
        print("âŒ æ— æ•°æ®ã€‚")
        return

    try:
        df = pd.read_csv(DATA_FILE_PATH, encoding='utf-8-sig')

        # 1. åŸºç¡€æ¸…æ´—
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df['impact_score'] = pd.to_numeric(df['impact_score'], errors='coerce').fillna(0)
        df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').fillna(0)
        df = df.dropna(subset=['date', 'content'])

        # 2. å‘¨æœ«è‡ªé€‚åº”çª—å£
        now = datetime.now()
        is_monday = now.weekday() == 0
        lookback_hours = 72 if is_monday else 24
        recent_df = df[df['date'] >= (now - timedelta(hours=lookback_hours))].copy()

        if recent_df.empty:
            print(f"ğŸ’¤ çª—å£å†…æ— æ•°æ®ã€‚")
            return

        # 3. è®¡ç®—è¡°å‡åˆ†
        recent_df['half_life'] = recent_df['date'].apply(get_dynamic_half_life)
        recent_df['hours_diff'] = (now - recent_df['date']).dt.total_seconds() / 3600.0
        recent_df['decayed_score'] = recent_df['impact_score'] * (
                    0.5 ** (recent_df['hours_diff'] / recent_df['half_life']))
        recent_df['freshness'] = recent_df['decayed_score'] / (recent_df['impact_score'] + 0.01)

        # 4. åŒå±‚èšåˆç»Ÿè®¡ (Tiered Aggregation)
        # å…ˆæŒ‰ä¸€çº§æ¿å—åˆ†ç»„
        level1_stats = []
        unique_sectors = recent_df['sector'].unique()

        for sector in unique_sectors:
            if sector in ["å…¶ä»–", "å…¨å±€", "nan"] or not isinstance(sector, str): continue

            sec_df = recent_df[recent_df['sector'] == sector]

            # ä¸€çº§æ¿å—å¼ºåº¦ (Top 3 å‡å€¼)
            l1_strength = sec_df['decayed_score'].sort_values(ascending=False).head(3).mean()
            if l1_strength < 4.0: continue  # è¿‡æ»¤å¼±æ¿å—

            # === äºŒçº§ç»†åˆ†æŒ–æ˜ (Drill Down) ===
            sub_stats = []
            unique_subs = sec_df['sub_sector'].unique()
            for sub in unique_subs:
                if not isinstance(sub, str) or sub == "é€šç”¨": continue
                sub_df = sec_df[sec_df['sub_sector'] == sub]
                # äºŒçº§å¼ºåº¦
                l2_strength = sub_df['decayed_score'].mean()
                l2_sentiment = sub_df['sentiment'].mean()
                sub_stats.append(f"{sub}(å¼º:{l2_strength:.1f}/æƒ…ç»ª:{l2_sentiment:.1f})")

            # å¦‚æœæ²¡æœ‰ç»†åˆ†ï¼Œå°±ç©ºç€
            sub_str = " | ".join(sub_stats) if sub_stats else "å…¨æ¿å—æ™®æ¶¨"

            level1_stats.append({
                'sector': sector,
                'strength': round(l1_strength, 2),
                'count': len(sec_df),
                'sub_details': sub_str,
                'top_news': sec_df.sort_values('decayed_score', ascending=False).iloc[0]['summary']
            })

        if not level1_stats: return

        # æ’åºå¹¶ç”Ÿæˆ Context
        stat_df = pd.DataFrame(level1_stats).sort_values('strength', ascending=False).head(5)
        sector_context = stat_df.to_string(index=False, columns=['sector', 'strength', 'sub_details'])

        # æå–é«˜åˆ†æ–°é—»è¯¦æƒ…
        detail_news = recent_df.sort_values('decayed_score', ascending=False).head(12)
        news_text = "\n".join([
                                  f"- [{row['decayed_score']:.1f}åˆ† | {row['sector']}-{row['sub_sector']}] {row['summary']} | é€»è¾‘:{row['logic']}"
                                  for _, row in detail_news.iterrows()])

        # 5. DeepSeek æˆ˜ç•¥ç”Ÿæˆ (ç»“æ„åŒ–æŒ‡ä»¤)
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=BASE_URL)
        prompt = f"""
        ä½ æ˜¯Aè‚¡é‡åŒ–åŸºé‡‘ç»ç†ã€‚ç°åœ¨æ˜¯{now.strftime('%A')}ç›˜å‰/åˆé—´ã€‚
        è¯·åŸºäºã€åŒå±‚æ¿å—ç»“æ„ã€‘åˆ†æèµ„é‡‘æµå‘ã€‚

        ã€ä¸€çº§æ¿å—å¼ºå¼±æ¦œ (Strength)ã€‘
        {sector_context}

        ã€æ ¸å¿ƒæƒ…æŠ¥ (å«äºŒçº§ç»†åˆ†)ã€‘
        {news_text}

        ã€ç­–ç•¥ç”Ÿæˆè¦æ±‚ã€‘
        1. **ç»“æ„åŒ–ä¸»çº¿**: æŒ‡å‡ºæœ€å¼ºçš„ä¸€çº§æ¿å—ï¼Œå¹¶**å¿…é¡»**ç‚¹å‡ºå…¶å†…éƒ¨æœ€å¼ºçš„ã€äºŒçº§ç»†åˆ†ã€‘ã€‚(ä¾‹å¦‚: "AIæ¿å—æœ€å¼ºï¼Œå†…éƒ¨èµ„é‡‘æ­£ä»åº”ç”¨ç«¯(æ¸¸æˆ)æµå‘ç¡¬ä»¶ç«¯(å…‰æ¨¡å—)")ã€‚
        2. **é¢„æœŸå·®åšå¼ˆ**: å¯»æ‰¾ `freshness` é«˜(æ–°æ¶ˆæ¯)ä½†å°šæœªä½“ç°åœ¨ `strength` ä¸Šçš„ç»†åˆ†é¢†åŸŸã€‚
        3. **é¿é›·æŒ‡å—**: æŒ‡å‡ºæƒ…ç»ª(sentiment)ä¸ºè´Ÿçš„ç»†åˆ†é¢†åŸŸã€‚
        4. **æ ‡çš„æ˜ å°„**: å¿…é¡»å¼•ç”¨æƒ…æŠ¥ä¸­çš„ `related_stocks`ã€‚

        æ ¼å¼ï¼šMarkdownï¼Œåˆ†ç‚¹é™ˆè¿°ï¼Œæ‹’ç»åºŸè¯ã€‚
        """

        response = client.chat.completions.create(
            model="deepseek-chat", messages=[{"role": "user", "content": prompt}], temperature=0.3
        )
        print("\n" + "=" * 40 + f"\nğŸ“Š DeepQuant ç»“æ„åŒ–å†…å‚\n" + "-" * 40)
        print(response.choices[0].message.content)
        print("=" * 40 + "\n")

    except Exception as e:
        print(f"âš ï¸ æ—¥æŠ¥ç”Ÿæˆå¤±è´¥: {e}")


# ================= ğŸ“¡ æŠ“å–æ¨¡å— =================
def fetch_cls_news(limit=20):
    timestamp = int(time.time())
    # rn = row number (æŠ“å–æ•°é‡)
    url = f"https://www.cls.cn/nodeapi/telegraphList?rn={limit}&_={timestamp}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.cls.cn/telegraph",
        "Host": "www.cls.cn",
        "Accept": "application/json, text/plain, */*",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
    }

    try:
        # å¢åŠ  timeout åˆ° 15ç§’ï¼Œé˜²æ­¢ç½‘ç»œæ…¢
        resp = requests.get(url, headers=headers, timeout=15)

        # ğŸš¨ è°ƒè¯•æ ¸å¿ƒï¼šå¦‚æœçŠ¶æ€ç ä¸æ˜¯ 200ï¼Œæ‰“å°å‡ºæ¥çœ‹çœ‹
        if resp.status_code != 200:
            print(f"âŒ è¯·æ±‚è¢«æ‹’ç»! çŠ¶æ€ç : {resp.status_code}")
            # print(f"   è¿”å›å†…å®¹: {resp.text[:100]}") # è°ƒè¯•æ—¶å¯è§£å¼€
            return []

        data = resp.json()

        # å…¼å®¹ä¸¤ç§è¿”å›ç»“æ„
        items = data.get('data', {}).get('roll_data') or data.get('data', {}).get('telegraph')

        if not items:
            print(f"âš ï¸ æ¥å£é€šäº†ä½†æ²¡æ•°æ®ã€‚è¿”å›ç»“æ„å¯èƒ½æ˜¯å˜äº†: {list(data.keys())}")
            return []

        raw_news = []
        for item in items:
            full_text = f"{item.get('title', '')} {item.get('content', '')}".strip()
            if not full_text: continue

            # è´¢è”ç¤¾æ—¶é—´æˆ³å¤„ç†
            ctime = item.get('ctime', int(time.time()))
            dt_str = datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M')

            raw_news.append({
                "id": str(item.get('id', hash(full_text))),
                "date": dt_str,
                "content": full_text
            })

        return raw_news

    except Exception as e:
        print(f"âŒ ç½‘ç»œ/è§£æè‡´å‘½é”™è¯¯: {e}")
        return []

 # ================= ğŸ§  æ ¸å¿ƒåˆ†æ (V14.1 æœ€ç»ˆå®šç¨¿ç‰ˆ) =================
def analyze_batch(news_list):
    if not news_list: return []

    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=BASE_URL)

    # [Context ç­–ç•¥]
    if 'MARKET_CONTEXT_MANUAL' in globals() and MARKET_CONTEXT_MANUAL:
        context_str = MARKET_CONTEXT_MANUAL
    elif MARKET_CONTEXT_BUFFER:
        context_str = "è¿‘æœŸçƒ­ç‚¹: " + " | ".join(MARKET_CONTEXT_BUFFER)
    else:
        context_str = "å¸‚åœºæƒ…ç»ªä¸­æ€§ï¼Œç­‰å¾…æ–¹å‘é€‰æ‹©"

    batch_input = [{"id": item['id'], "content": item['content']} for item in news_list]

    prompt = f"""
    ã€èƒŒæ™¯ã€‘å¸‚åœºçŠ¶æ€ï¼š{context_str}
    ã€äº§ä¸šé“¾å›¾è°±ã€‘ï¼š{SECTOR_KNOWLEDGE}
    ã€è§’è‰²ã€‘Aè‚¡ç­–ç•¥åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ç©¿é€å™ªéŸ³ï¼Œè¯†åˆ«ã€é¢„æœŸå·®ã€‘ä¸ã€åšå¼ˆä»·å€¼ã€‘ã€‚

    ã€æ ¸å¿ƒé“å¾‹ (æŒ‰ç±»å‹åŒ¹é…)ã€‘
    1.  ã€æ”¿ç­–ç±»ã€‘ï¼šéµå¾ª"æ”¿ç­–å³å‘½ä»¤"ã€‚
        - **å®šæ€§**ï¼šåŒºåˆ†å®æ‹›(æ”¹å˜èµ„é‡‘/è§„åˆ™)ä¸è™šæ‹›(å£å·)ã€‚
        - **åšå¼ˆ**ï¼šå¿…é¡»ç»“åˆ **{context_str}** åˆ¤æ–­ã€‚å†°ç‚¹å‡ºåˆ©å¥½=é›ªä¸­é€ç‚­ï¼›é«˜ä½å‡ºåˆ©ç©º=é™æ¸©æ‰“å‡»ã€‚
    2.  ã€æµ·å¤–æ˜ å°„ã€‘ï¼šæåŠå°ç§¯ç”µ/è‹±ä¼Ÿè¾¾/ç‰¹æ–¯æ‹‰/OpenAIç­‰å›½å¤–å·¨å¤´çš„é‡ç£…æ¶ˆæ¯æ—¶ï¼Œ**å¿…é¡»**å…³è”Aè‚¡å¯¹åº”äº§ä¸šé“¾åŠAè‚¡å¯¹åº”ã€äºŒçº§ç»†åˆ†ã€‘(å¦‚åŠå¯¼ä½“è®¾å¤‡/å…‰æ¨¡å—/æ±½é…)ï¼Œè§†ä¸ºé«˜æƒé‡æŒ‡å¼•ã€‚
    3.  ã€ä¸ªè‚¡å¾®è§‚ã€‘ï¼š
        - **ä¸šç»©æ—¶æœº**ï¼šé¢„å‘ŠæœŸå†…å¢é•¿=æ˜ç‰Œ(ä½åˆ†)ï¼›éé¢„å‘ŠæœŸçªå‘=é¢„æœŸå·®(é«˜åˆ†)ã€‚
        - **åˆåŒ/è®¢å• (é‡åŒ–æ ‡å°º)**ï¼š
            *   **é«˜èƒ½ (7-8åˆ†)**ï¼šå ä¸Šå¹´è¥æ”¶æ¯”é‡ **>30%**ã€‚
            *   **ä¸­æ€§ (5-6åˆ†)**ï¼šå ä¸Šå¹´è¥æ”¶æ¯”é‡ **5%-30%**ã€‚
            *   **å¾®å¼± (0-4åˆ†)**ï¼šå ä¸Šå¹´è¥æ”¶æ¯”é‡ **<5%** æˆ–æœªæŠ«éœ²é‡‘é¢ã€‚
        - **æŠ€æœ¯çªç ´**ï¼šéœ€æ˜ç¡®â€œè·æƒå¨è®¤è¯â€æˆ–â€œè·é‡äº§è®¢å•â€ï¼Œå¦åˆ™è§†ä¸ºâ€œè½¯ä¿¡æ¯â€æ‰“æŠ˜å¤„ç†ã€‚
        - **èµ„é‡‘åŠ¨ä½œ**ï¼šæ³¨é”€å¼å›è´­ > çœŸé‡‘å¢æŒ > æ‰¿è¯ºä¸å‡æŒ > å£å¤´å£å·ã€‚

    ã€è¯„åˆ†æ ‡å‡† (0-10) - æ¢¯åº¦ä¼˜åŒ–ã€‘
    - 9-10åˆ†ã€æ ¸å¼¹/ç»“æ„æ€§é¢ è¦†ã€‘ï¼šæé«˜æ„å¤–æ€§ã€‚å¦‚ï¼šå°èŠ±ç¨ã€é™åˆ¶é‡åŒ–ã€å®æ§äººè¢«æŠ“ã€éé¢„å‘ŠæœŸä¸šç»©æš´é›·/æš´å¢ç­‰ã€‚
    - 7-8åˆ† ã€é«˜èƒ½/å¼ºé©±åŠ¨ã€‘ï¼šå®è´¨æ€§åˆ©å¥½ã€‚å¦‚ï¼šæµ·å¤–æ˜ å°„çˆ†å‘ã€**è¥æ”¶å æ¯”>30%å¤§è®¢å•**ã€è¡Œä¸šå„æ–­æ€§æŠ€æœ¯çªç ´ç­‰ã€‚
    - 6åˆ†   ã€æ˜¾è‘—/è¶…é¢„æœŸã€‘ï¼šæ˜ç¡®çš„åˆ©å¥½ï¼Œä¸”ç•¥è¶…å¸‚åœºé¢„æœŸã€‚
    - 4-5åˆ† ã€å…³æ³¨/æ˜ç‰Œã€‘ï¼šä¿¡æ¯çœŸå®ä½†å½±å“å¾®å¼±/å·²å…‘ç°ã€‚å¦‚ï¼š**è¥æ”¶å æ¯”5-30%çš„ä¸­ç­‰åˆåŒ**ã€é¢„å‘ŠæœŸå†…è¾¾æ ‡é¢„å¢ã€‚
    - 0-3åˆ† ã€å™ªéŸ³/åƒåœ¾ã€‘ï¼š**è¥æ”¶å æ¯”<5%å°åˆåŒ**ã€çº¯è¡Œæƒ…æ’­æŠ¥ã€æ— æ¥æºä¼ é—»ã€æ— å…³æµ·å¤–äº‹ä»¶ã€‚

    ã€è¾“å…¥æ–°é—»ã€‘
    {json.dumps(batch_input, ensure_ascii=False)}

    ã€è¾“å‡ºJSONåˆ—è¡¨ã€‘
    - `id`: åŸæ ·è¿”å›
    - `score`: æ•´æ•°(0-10)
    - `sentiment`: -1.0(ç©º) ~ 1.0(å¤š)ã€‚
    - `summary`: 8å­—å†…æ ¸å¿ƒæ ‡ç­¾
    - `sector`: **ä¸€çº§å¤§ç±»** (å¦‚: äººå·¥æ™ºèƒ½, åŠå¯¼ä½“, æ±½è½¦äº§ä¸šé“¾)ã€‚æ”¿ç­–ç±»æ— ç‰¹å®šæ¿å—å¡«"å…¨å±€"ã€‚
    - `sub_sector`: **äºŒçº§ç»†åˆ†** (å¦‚: AIç¡¬ä»¶, æ¸¸æˆä¼ åª’, åŠå¯¼ä½“è®¾å¤‡)ã€‚è‹¥æ— ç»†åˆ†å¡«"é€šç”¨"ã€‚
    - `type`: Policy/Micro/Industry/Noise
    - `impact_horizon`: Immediate/Short/Medium
    - `key_trigger`: æ”¿ç­–/ä¸šç»©/åˆåŒ/å‡æŒ/å›è´­/æ˜ å°„/å…¶ä»–
    - `related_stocks`: ["å…¬å¸å"]
    - `logic`: ã€å…³é”®ã€‘ä¸€å¥çŠ€åˆ©ç‚¹è¯„ã€‚
       - **åˆåŒç±»**ï¼šå¿…é¡»æ³¨æ˜"è¥æ”¶å æ¯”çº¦xx%"ï¼Œä»¥æ­¤ä½œä¸ºè¯„åˆ†ä¾æ®ã€‚
       - **æ”¿ç­–ç±»**ï¼šç‚¹æ˜å…·ä½“å—å½±å“çš„ç»†åˆ†é¢†åŸŸ (å¦‚"æ•°æ®è¦ç´ å…¥è¡¨ï¼Œåˆ©å¥½æ•°å­—ç»æµ")ã€‚
       - **å™ªéŸ³ç±»**ï¼š(0-3åˆ†) ç›´æ¥æ³¨æ˜"æ— å¢é‡ä¿¡æ¯"ã€‚
    """
    raw_content = "ï¼ˆæœªè·å–åˆ°å†…å®¹ï¼‰"

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1, max_tokens=4000
        )
        raw_content = response.choices[0].message.content

        # æ¸…æ´—
        cleaned_content = clean_json_string(raw_content)

        # è§£æ
        parsed_data = json.loads(cleaned_content)

        if isinstance(parsed_data, dict):
            parsed_data = [parsed_data]

        return parsed_data


    except json.JSONDecodeError:

        print("\nâŒ JSON è§£æå¤±è´¥ï¼DeepSeek è¿”å›äº†é JSON å†…å®¹ã€‚")

        print("ğŸ” æ¡ˆå‘ç°åœº (Raw Content):")

        print("-" * 20)

        print(raw_content)  # <--- è¿™è¡Œä¼šå‘Šè¯‰ä½ çœŸç›¸

        print("-" * 20)

        return []

    except Exception as e:

        print(f"âš ï¸ AI è°ƒç”¨å…¶ä»–æŠ¥é”™: {e}")

        return []


# ================= ğŸš¨ æ¿å—å…±æŒ¯é›·è¾¾ (æ ¸å¿ƒå‡çº§) =================
def check_sector_resonance(new_items):
    """
    ç»´æŠ¤ 1 å°æ—¶çš„æ»‘åŠ¨çª—å£ï¼Œæ£€æµ‹ã€äºŒçº§ç»†åˆ†ã€‘çš„èµ„é‡‘å…±æŒ¯
    """
    global SECTOR_HISTORY_BUFFER
    now = time.time()

    # 1. å°†æ–°æ•°æ®åŠ å…¥å†å²ç¼“å­˜ (åªå­˜æœ‰æ•ˆæ•°æ®)
    for item in new_items:
        if item.get('sub_sector') and item['sub_sector'] != 'é€šç”¨':
            SECTOR_HISTORY_BUFFER.append({
                'time': now,
                'sector': item['sector'],
                'sub_sector': item['sub_sector'],
                'score': item.get('score', 0),
                'summary': item['summary']
            })

    # 2. æ¸…ç†è¶…è¿‡ 1 å°æ—¶çš„æ•°æ® (æ»‘åŠ¨çª—å£)
    SECTOR_HISTORY_BUFFER = [x for x in SECTOR_HISTORY_BUFFER if now - x['time'] < 3600]

    # 3. ç»Ÿè®¡æ•°æ® (æŒ‰ äºŒçº§ç»†åˆ† èšåˆ)
    # ç»“æ„: {'å…‰æ¨¡å—': {'total': 3, 'high': 2, 'parent': 'AI'}}
    stats = defaultdict(lambda: {'total': 0, 'high_score': 0, 'parent': '', 'titles': []})

    for x in SECTOR_HISTORY_BUFFER:
        sub = x['sub_sector']
        stats[sub]['total'] += 1
        stats[sub]['parent'] = x['sector']
        if x['score'] >= 7:
            stats[sub]['high_score'] += 1
        stats[sub]['titles'].append(x['summary'])

    # 4. è§¦å‘è­¦æŠ¥
    # è§„åˆ™: 1å°æ—¶å†…ï¼Œè¯¥ç»†åˆ†é¢†åŸŸæ–°é—»æ•° >=2 ä¸” è‡³å°‘æœ‰1æ¡æ˜¯é«˜èƒ½æ–°é—»
    # (ç»†åˆ†é¢†åŸŸæ–°é—»å°‘ï¼Œé˜ˆå€¼æ¯”ä¸€çº§æ¿å—è¦ä½ä¸€ç‚¹ï¼Œçµæ•åº¦è¦é«˜)
    for sub, data in stats.items():
        if data['total'] >= 2 and data['high_score'] >= 1:
            print(f"\nğŸš¨ğŸš¨ ã€èµ„é‡‘å…±æŒ¯è­¦æŠ¥ã€‘ >>> {data['parent']} - {sub} <<<")
            print(f"   ğŸ”¥ 1å°æ—¶å†…çˆ†å‘ {data['total']} æ¡æ¶ˆæ¯ (é«˜èƒ½: {data['high_score']})")
            print(f"   ğŸ“ çº¿ç´¢: {' | '.join(list(set(data['titles'])))}")
            print("-" * 30)


# ================= ğŸš€ ä¸»æµç¨‹ (ä¿®å¤é™é»˜å‡æ­»ç‰ˆ) =================
def run_pipeline(is_first_run=False):
    global SEEN_NEWS_BUFFER

    # 1. æŠ“å–
    fetch_limit = 100 if is_first_run else 20
    if is_first_run: print(f"ğŸš€ ç³»ç»Ÿå†·å¯åŠ¨ï¼šå›æº¯å†å²æ•°æ® (Top {fetch_limit})...")

    raw = fetch_cls_news(limit=fetch_limit)
    if not raw:
        if not is_first_run: print(f"[{datetime.now().strftime('%H:%M')}] âš ï¸ æºå¤´æ— æ•°æ®")
        return

    # 2. å¢é‡ç­›é€‰
    batch = []
    skipped_count = 0
    for item in raw:
        if item['content'] in SEEN_NEWS_BUFFER:
            skipped_count += 1
            continue
        if any(n in item['content'] for n in NOISE_KEYWORDS): continue
        if len(item['content']) < 8: continue
        batch.append(item)

    # çŠ¶æ€æ‰“å°
    timestamp = datetime.now().strftime('%H:%M')
    if is_first_run:
        print(f"[{timestamp}] å›æº¯ç»“æŸ | æŠ“å–:{len(raw)} | å·²å­˜æ—§é—»:{skipped_count} | æ–°å¢å¾…åˆ†æ:{len(batch)}")
    elif not batch:
        return
    else:
        print(f"[{timestamp}] ğŸ” å‘ç° {len(batch)} æ¡æ–°çº¿ç´¢ï¼Œå‡†å¤‡åˆ†æ‰¹åˆ†æ...")

    # 3. åˆ†æ‰¹ AI åˆ†æ (Chunking) - æ ¸å¿ƒä¿®å¤ç‚¹
    # æ¯æ¬¡åªå–‚ 5 æ¡ï¼Œé˜²æ­¢ Token çˆ†ç‚¸å¯¼è‡´ JSON æˆªæ–­
    CHUNK_SIZE = 5
    final_data = []

    for i in range(0, len(batch), CHUNK_SIZE):
        chunk = batch[i: i + CHUNK_SIZE]
        print(f"   â˜• æ­£åœ¨åˆ†æç¬¬ {i + 1}-{min(i + CHUNK_SIZE, len(batch))} æ¡...")

        # è°ƒç”¨ AI
        results = analyze_batch(chunk)

        # å»ºç«‹æ˜ å°„
        result_map = {str(res['id']): res for res in results}

        for item in chunk:
            SEEN_NEWS_BUFFER.add(item['content'])
            res = result_map.get(item['id'])

            if res:
                score = res.get('score', 0)
                # è¿‡æ»¤å™ªéŸ³ (0-4åˆ†)
                if score > 4:
                    item.update(res)
                    final_data.append(item)
                    print(
                        f"      âœ… [{score}åˆ† | {res.get('sector', '?')}-{res.get('sub_sector', '?')}] {res.get('summary', '')}")
                else:
                    print(f"      ğŸ—‘ï¸ [å™ªéŸ³] {res.get('summary', 'æ— ä»·å€¼')}")
            else:
                # å¦‚æœ AI è¿”å›çš„åˆ—è¡¨é‡Œæ²¡è¿™ä¸ª IDï¼Œè¯´æ˜åˆ†ææ¼äº†æˆ–è€…å‡ºé”™
                print(f"      âš ï¸ åˆ†æé—æ¼: {item['content'][:10]}...")

        # æ‰¹æ¬¡é—´ç¨å¾®æ­‡ä¸€ä¸‹ï¼Œé˜²æ­¢ API QPS é™åˆ¶
        time.sleep(1)

    # 4. å†…å­˜ç»´æŠ¤
    if len(SEEN_NEWS_BUFFER) > 2000:
        SEEN_NEWS_BUFFER = set(list(SEEN_NEWS_BUFFER)[-2000:])

    # 5. åå¤„ç†ä¸å­˜å‚¨
    if final_data:
        check_sector_resonance(final_data)

        df_new = pd.DataFrame(final_data)
        file_exists = os.path.exists(DATA_FILE_PATH) and os.path.getsize(DATA_FILE_PATH) > 0

        try:
            df_new.to_csv(DATA_FILE_PATH, mode='a', header=not file_exists, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ æœ¬è½®å…¥åº“ {len(final_data)} æ¡æƒ…æŠ¥")
        except:
            print("   âŒ å†™å…¥å¤±è´¥ï¼Œè¯·å…³é—­ Excel")



if __name__ == "__main__":
    if "sk-" not in DEEPSEEK_API_KEY:
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆå¡«å…¥ DeepSeek API Key")
    else:
        print(f"\nğŸ“¡ DeepQuant V14.1 (ç»“æ„åŒ–èµ„é‡‘æµç‰ˆ) å¯åŠ¨...")
        print(f"ğŸ¯ ç›‘æ§é¢‘ç‡: {POLLING_INTERVAL} åˆ†é’Ÿ/è½®")

        # 1. æ¢å¤è®°å¿†
        init_memory()

        # 2. ç«‹å³è·‘ä¸€æ¬¡
        run_pipeline()

        # 3. è®¾å®šå®šæ—¶ä»»åŠ¡
        schedule.every(POLLING_INTERVAL).minutes.do(run_pipeline)

        # è®¾å®šç›˜å‰/åˆé—´å†…å‚ç”Ÿæˆ
        schedule.every().day.at("08:30").do(generate_daily_brief)
        schedule.every().day.at("12:00").do(generate_daily_brief)

        # 4. å®ˆæŠ¤è¿›ç¨‹
        while True:
            try:
                schedule.run_pending()
                time.sleep(1)
            except KeyboardInterrupt:
                print("\nğŸ›‘ä»¥æ­¤åœæ­¢æœåŠ¡")
                break
            except Exception as e:
                print(f"\nâŒ ä¸»å¾ªç¯å¼‚å¸¸: {e} (5ç§’åé‡è¯•)")
                time.sleep(5)