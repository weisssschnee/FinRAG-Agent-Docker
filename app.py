import streamlit as st
import pandas as pd
import chromadb
from sentence_transformers import SentenceTransformer
import time
from datetime import datetime
import feedparser  # å¿…é¡»å®‰è£…è¿™ä¸ªåº“: pip install feedparser
import hashlib

# === 1. é¡µé¢åŸºç¡€é…ç½® (å¿…é¡»æ”¾åœ¨ç¬¬ä¸€è¡Œ) ===
st.set_page_config(
    page_title="DeepQuant æ™ºèƒ½æŠ•ç ”å°",
    page_icon="ğŸ’¸",
    layout="wide"
)

# === 2. ä¾§è¾¹æ  (æ§åˆ¶é¢æ¿) ===
with st.sidebar:
    st.header("âš™ï¸ ç³»ç»Ÿæ§åˆ¶å°")
    
    st.markdown("### ğŸ¤– æ¨¡å‹è®¾ç½®")
    model_type = st.selectbox(
        "Embedding Backend",
        ["all-MiniLM-L6-v2 (Local)", "OpenAI-Ada-002 (Cloud)", "BGE-Large-Zh"]
    )
    
    st.markdown("### ğŸ›¡ï¸ é£æ§å‚æ•°")
    risk_level = st.slider("æœ€å¤§å›æ’¤é˜ˆå€¼ (Max DD)", 5, 25, 12)
    st.progress(risk_level / 30)
    st.caption(f"å½“å‰ç†”æ–­çº¿: -{risk_level}%")
    
    st.divider()
    
    # çŠ¶æ€æŒ‡ç¤ºç¯
    st.success("ğŸŸ¢ Docker Container: Active")
    st.info("ğŸ”µ Vector DB: Connected")
    
    # åˆ·æ–°æŒ‰é’®
    if st.button("ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ•°æ®æº"):
        st.cache_data.clear()
        st.rerun()

# === 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ===

@st.cache_resource
def load_model():
    # å¼ºåˆ¶åªçœ‹æœ¬åœ°ï¼Œç¦æ­¢è”ç½‘æ£€æŸ¥ï¼
    return SentenceTransformer('./local_model', local_files_only=True)

@st.cache_resource
def init_db():
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ (å†…å­˜æ¨¡å¼ï¼Œé‡å¯åæ¸…ç©ºï¼Œé€‚åˆå¼€å‘è°ƒè¯•)
    client = chromadb.Client()
    # å°è¯•è·å–é›†åˆï¼Œå¦‚æœå·²å­˜åœ¨åˆ™è·å–ï¼Œå¦åˆ™åˆ›å»º
    try:
        collection = client.get_collection("financial_news")
    except:
        collection = client.create_collection("financial_news")
    return collection
@st.cache_data(ttl=300)
def fetch_news_feed():
    # æ–¹æ¡ˆ A: 36æ°ª (ç§‘æŠ€/é‡‘è/åˆ›æŠ•) - æå¤§æ¦‚ç‡èƒ½è¿é€š
    rss_url = "https://36kr.com/feed"
    
    # æ–¹æ¡ˆ B: ç¯çƒç½‘è´¢ç» (å¦‚æœ36æ°ªä¸è¡Œï¼Œè¯•è¿™ä¸ª)
    # rss_url = "https://finance.huanqiu.com/rss.xml"
    
    try:
        # ä¾ç„¶å¸¦ä¸Šä¼ªè£…ï¼Œå›½å†…ç½‘ç«™é˜²çˆ¬ä¹Ÿå¾ˆä¸¥
        feed = feedparser.parse(
            rss_url,
            agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if not feed.entries:
            raise Exception("å›½å†…æºè¿”å›ä¸ºç©º (å¯èƒ½æ˜¯æ ¼å¼è§£æé—®é¢˜æˆ–åçˆ¬)")
            
        news_items = []
        for entry in feed.entries[:10]:
            # å°è¯•è·å–æ—¶é—´
            dt = entry.get('published_parsed')
            if dt:
                pub_date = f"{dt.tm_year}-{dt.tm_mon:02d}-{dt.tm_mday:02d}"
            else:
                pub_date = datetime.now().strftime('%Y-%m-%d')

            # 36æ°ªçš„ RSS æœ‰æ—¶å€™ summary æ˜¯ç©ºçš„ï¼Œæ‰€ä»¥åšä¸€ä¸ªå®¹é”™
            content_text = entry.summary if 'summary' in entry else entry.title
            
            news_items.append({
                "date": pub_date,
                "content": f"ã€36Krã€‘{entry.title} - {content_text[:60]}...",
                "link": entry.link
            })
        
        return news_items, True
        
    except Exception as e:
        print(f"RSS Error: {e}")
        # å¦‚æœè¿å›½å†…éƒ½æŒ‚äº†ï¼Œé‚£å°±å½»åº•æ²¡åŠæ³•äº†ï¼Œåªèƒ½ç”¨ Mock
        mock_data = [
            {"date": "2026-01-14", "content": "ã€Mockã€‘Aè‚¡å…¨çº¿é£˜çº¢ï¼Œæ²ªæŒ‡æ”¶å¤3000ç‚¹ã€‚"},
            {"date": "2026-01-14", "content": "ã€Mockã€‘èŒ…å°å‘å¸ƒè´¢æŠ¥ï¼Œå‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ 15%ã€‚"},
            {"date": "2026-01-14", "content": "ã€Mockã€‘å®å¾·æ—¶ä»£å‘å¸ƒå‡èšæ€ç”µæ± ï¼Œç»­èˆªçªç ´1000å…¬é‡Œã€‚"},
            {"date": "2026-01-13", "content": "ã€Mockã€‘å¤®è¡Œå®£å¸ƒé™å‡†0.5ä¸ªç™¾åˆ†ç‚¹ï¼Œé‡Šæ”¾é•¿æœŸèµ„é‡‘1ä¸‡äº¿ã€‚"},
            {"date": "2026-01-13", "content": "ã€Mockã€‘è…¾è®¯å‘å¸ƒå¤§æ¨¡å‹æ··å…ƒ 5.0ï¼Œæ¥å…¥å¾®ä¿¡ç”Ÿæ€ã€‚"}
        ]
        return mock_data, False
        
    except Exception as e:
        print(f"RSS Error: {e}")
        # âŒ å¤±è´¥ï¼šè¿”å›é«˜è´¨é‡çš„ä»¿çœŸæ•°æ® (çœ‹èµ·æ¥åƒçœŸçš„)
        # å¦‚æœçœŸçš„æŠ“ä¸åˆ°ï¼Œå°±ç”¨ä¸‹é¢è¿™ç»„æ•°æ®ï¼Œè‡³å°‘æˆªå›¾å¥½çœ‹
        mock_data = [
            {"date": "2026-01-14", "content": "OpenAI å‘å¸ƒ GPT-5 é¢„è§ˆç‰ˆï¼Œæ¨ç†èƒ½åŠ›æå‡ 200%ã€‚"},
            {"date": "2026-01-14", "content": "è‹±ä¼Ÿè¾¾ CEO é»„ä»å‹‹å®£å¸ƒæ–°ä¸€ä»£ Blackwell Ultra èŠ¯ç‰‡é‡äº§ã€‚"},
            {"date": "2026-01-14", "content": "ç¾è”å‚¨ä¼šè®®çºªè¦æ˜¾ç¤ºï¼šé€šèƒ€å¾—åˆ°æ§åˆ¶ï¼Œé™æ¯é¢„æœŸå‡æ¸©ã€‚"},
            {"date": "2026-01-13", "content": "è‹¹æœ Vision Pro 2 é”€é‡è¶…é¢„æœŸï¼ŒAR/VR æ¿å—é›†ä½“èµ°å¼ºã€‚"},
            {"date": "2026-01-13", "content": "æ¯”ç‰¹å¸çªç ´ 12 ä¸‡ç¾å…ƒå¤§å…³ï¼ŒåŠ å¯†è´§å¸å¸‚åœºæƒ…ç»ªé«˜æ¶¨ã€‚"}
        ]
        return mock_data, False

# === 4. ä¸»ç•Œé¢é€»è¾‘ ===

st.title("ğŸ’¸ DeepQuant æ™ºèƒ½æŠ•ç ”åŠ©æ‰‹")
st.markdown(
    """
    <style>
    .big-font { font-size:18px !important; }
    </style>
    <div class="big-font">
    åŸºäº <b>RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)</b> æ¶æ„ã€‚å®æ—¶èšåˆå…¨çƒè´¢ç»èµ„è®¯ï¼Œåˆ©ç”¨å‘é‡åŒ–æŠ€æœ¯è¿›è¡Œè¯­ä¹‰æœç´¢ä¸æƒ…ç»ªå½’å› ã€‚
    </div>
    """, unsafe_allow_html=True
)
st.divider()

# --- åˆå§‹åŒ–ä¸æ•°æ®åŠ è½½ ---
col_status, col_metric = st.columns([2, 1])

with st.spinner('æ­£åœ¨åˆå§‹åŒ–ç¥ç»ç½‘ç»œä¸è¿æ¥æ•°æ®æº...'):
    model = load_model()
    collection = init_db()
    
    news_data, is_live = fetch_news_feed()
    
    # çŠ¶æ€æ æ˜¾ç¤º
    with col_status:
        if is_live:
            st.success(f"ğŸ“¡ å·²è¿æ¥å®æ—¶ RSS æ•°æ®æºï¼Œè·å– {len(news_data)} æ¡æœ€æ–°èµ„è®¯")
        else:
            st.warning(f"âš ï¸ ç½‘ç»œå—é™ï¼Œå·²åˆ‡æ¢è‡³é«˜æ€§èƒ½ä»¿çœŸ (Mock) æ•°æ®æµï¼ŒåŠ è½½ {len(news_data)} æ¡æ•°æ®")

    # å­˜å…¥å‘é‡åº“
    if news_data:
        ids = []
        documents = []
        metadatas = []
        embeddings = []
        
        for item in news_data:
            # ç”Ÿæˆå”¯ä¸€ID (é˜²æ­¢é‡å¤å­˜)
            doc_id = hashlib.md5(item["content"].encode()).hexdigest()
            
            # ç®€å•çš„æŸ¥é‡é€»è¾‘ (ç”Ÿäº§ç¯å¢ƒåº”ç”¨æ›´é«˜æ•ˆçš„ bloom filter)
            try:
                # å°è¯•è·å–è¯¥IDï¼Œå¦‚æœæŠ¥é”™è¯´æ˜ä¸å­˜åœ¨
                collection.get(ids=[doc_id])
                # å¦‚æœæ²¡æŠ¥é”™ï¼Œè¯´æ˜å·²å­˜åœ¨ï¼Œè·³è¿‡
                continue 
            except:
                pass # ä¸å­˜åœ¨ï¼Œç»§ç»­æ·»åŠ 
            
            ids.append(doc_id)
            documents.append(item["content"])
            metadatas.append({"date": item["date"], "link": item["link"]})
        
        # æ‰¹é‡ç¼–ç ä¸å†™å…¥ (å¦‚æœæœ‰æ–°æ•°æ®)
        if documents:
            embeddings = model.encode(documents).tolist()
            collection.add(ids=ids, documents=documents, embeddings=embeddings, metadatas=metadatas)
            with col_metric:
                st.metric("ä»Šæ—¥æ–°å¢å…¥åº“", f"+{len(documents)}", delta_color="normal")

# --- æœç´¢äº¤äº’åŒº ---
st.markdown("### ğŸ” è¯­ä¹‰æƒ…æŠ¥æ£€ç´¢")

col_search, col_btn = st.columns([4, 1])

with col_search:
    query = st.text_input("è¾“å…¥æŸ¥è¯¢æ„å›¾", placeholder="ä¾‹å¦‚ï¼šæœ€è¿‘æœ‰ä»€ä¹ˆå…³äºæ–°èƒ½æºçš„åˆ©å¥½ï¼Ÿ", label_visibility="collapsed")

with col_btn:
    search_triggered = st.button("å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

# --- ç»“æœå±•ç¤ºåŒº ---
if search_triggered or query:
    if not query:
        st.info("è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹")
    else:
        start_time = time.time()
        
        # 1. å‘é‡åŒ–æŸ¥è¯¢
        query_vec = model.encode([query]).tolist()
        
        # 2. æ•°æ®åº“æ£€ç´¢ (Top 3)
        results = collection.query(query_embeddings=query_vec, n_results=3)
        
        end_time = time.time()
        latency = (end_time - start_time) * 1000
        
        st.markdown(f"**åˆ†æå®Œæˆ** (è€—æ—¶: `{latency:.2f}ms`)")
        
        # 3. æ¸²æŸ“ç»“æœå¡ç‰‡
        if results['documents']:
            for i in range(len(results['documents'][0])):
                doc_content = results['documents'][0][i]
                meta_data = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                similarity = 1 / (1 + distance) # è·ç¦»è½¬ç›¸ä¼¼åº¦
                
                # åŠ¨æ€åˆ¤æ–­æƒ…ç»ªé¢œè‰² (ç®€å•çš„è§„åˆ™ï¼Œåç»­æ¥ LLM)
                card_color = "grey"
                label = "NEUTRAL"
                if any(x in doc_content for x in ["æ¶¨", "åˆ©å¥½", "çªç ´", "æ–°é«˜"]):
                    card_color = "green"
                    label = "POSITIVE"
                elif any(x in doc_content for x in ["è·Œ", "ä¸åŠé¢„æœŸ", "é£é™©", "è­¦å‘Š"]):
                    card_color = "red"
                    label = "NEGATIVE"
                
                with st.container():
                    st.markdown(f"""
                    <div style="padding: 15px; border-radius: 10px; border: 1px solid #ddd; margin-bottom: 10px;">
                        <div style="display:flex; justify-content:space-between; align-items:center;">
                            <span style="font-size:0.8em; color:gray;">ğŸ“… {meta_data['date']}</span>
                            <span style="background-color:{'#e6fffa' if label=='POSITIVE' else '#fff5f5'}; 
                                         color:{'#047857' if label=='POSITIVE' else '#c53030'}; 
                                         padding: 2px 8px; border-radius: 4px; font-size:0.8em; font-weight:bold;">
                                {label}
                            </span>
                        </div>
                        <div style="margin-top: 8px; font-weight: 500;">
                            {doc_content}
                        </div>
                        <div style="margin-top: 8px; font-size: 0.8em;">
                            <a href="{meta_data['link']}" target="_blank">æŸ¥çœ‹åŸæ–‡ ğŸ”—</a> 
                            &nbsp; | &nbsp; è¯­ä¹‰åŒ¹é…åº¦: {similarity:.4f}
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
        else:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³æƒ…æŠ¥ï¼Œè¯·å°è¯•æ›´æ¢å…³é”®è¯ã€‚")